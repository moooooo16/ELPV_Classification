{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mobilenet_v3_large, SMOTE, ReLu, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision.models import swin_s\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elpv_reader import load_dataset\n",
    "images, proba, types = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 ... 0 1 3]\n"
     ]
    }
   ],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train_set, test_set, train_lables, test_lables = \\\n",
    "train_test_split(images, proba, test_size=0.25,random_state=42)\n",
    "\n",
    "my_mapping = {0.0:0, 0.3333333333333333:1, 0.6666666666666666:2, 1.0:3}\n",
    "train_lables_new = []\n",
    "for i in range(len(train_set)):\n",
    "    train_lables_new.append(my_mapping[train_lables[i]])\n",
    "train_lables_new = np.array(train_lables_new)\n",
    "print(train_lables_new)\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_lables[i] = my_mapping[test_lables[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0.0 has: 1136\n",
      "The 0.33 has: 222\n",
      "The 0.66 has: 75\n",
      "The 1.0 has: 535\n"
     ]
    }
   ],
   "source": [
    "count_list = [0,0,0,0]\n",
    "for i in range(len(train_set)):\n",
    "    if train_lables_new[i] == 0:\n",
    "        count_list[0] += 1\n",
    "    if train_lables_new[i] == 1:\n",
    "        count_list[1] += 1\n",
    "    if train_lables_new[i] == 2:\n",
    "        count_list[2] += 1\n",
    "    if train_lables_new[i] == 3:\n",
    "        count_list[3] += 1 \n",
    "for i, j in zip(['The 0.0 has:', 'The 0.33 has:', 'The 0.66 has:', 'The 1.0 has:'], count_list):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(train_set)\n",
    "new_set_2d = train_set.reshape(train_set.shape[0], -1)\n",
    "# sampling_strategy = {0: 1136, 1: 800, 2: 800, 3: 800}\n",
    "# oversample = SMOTE(sampling_strategy=sampling_strategy)\n",
    "oversample = SMOTE(sampling_strategy='not majority')\n",
    "new_set1, new_labels1 =  oversample.fit_resample(new_set_2d, train_lables_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4544, 90000)\n",
      "(4544, 300, 300)\n",
      "4544\n",
      "The 0.0 has: 1136\n",
      "The 0.33 has: 1136\n",
      "The 0.66 has: 1136\n",
      "The 1.0 has: 1136\n"
     ]
    }
   ],
   "source": [
    "print(new_set1.shape)\n",
    "new_set1_3d = new_set1.reshape((-1, 300, 300))\n",
    "print(new_set1_3d.shape)\n",
    "\n",
    "# for i in range(len(new_labels1)):\n",
    "#     print(new_labels1[i])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(new_set1_3d)):\n",
    "    # plt.imshow(new_set1_3d[i], cmap='gray')\n",
    "    # plt.show()\n",
    "    # print(new_labels1[i])\n",
    "    count += 1\n",
    "print(count)\n",
    "\n",
    "count_list = [0,0,0,0]\n",
    "for i in range(len(new_set1_3d)):\n",
    "    if new_labels1[i] == 0:\n",
    "        count_list[0] += 1\n",
    "    if new_labels1[i] == 1:\n",
    "        count_list[1] += 1\n",
    "    if new_labels1[i] == 2:\n",
    "        count_list[2] += 1\n",
    "    if new_labels1[i] == 3:\n",
    "        count_list[3] += 1 \n",
    "for i, j in zip(['The 0.0 has:', 'The 0.33 has:', 'The 0.66 has:', 'The 1.0 has:'], count_list):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=False):\n",
    "        self.images = images\n",
    "        self.label_mapping = {0: np.eye(4)[0], 1: np.eye(4)[1],\\\n",
    "                              2: np.eye(4)[2], 3: np.eye(4)[3]}\n",
    "        self.labels = [self.label_mapping[label] for label in labels]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        # 如果图像为灰度图像，将其复制到三个通道，转换为 RGB 图像\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "custom_train_dataset = CustomDataset(new_set1_3d, new_labels1, transform)\n",
    "custom_test_dataset = CustomDataset(test_set, test_lables, transform)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(custom_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(custom_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torchtest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  \n",
    "\n",
    "model = mobilenet_v3_large(weights=True)\n",
    "\n",
    "classifier1 = nn.Sequential(OrderedDict([('fc1', nn.Linear(960, 4)),\n",
    "                           ('output', nn.ReLU())\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier1\n",
    "\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:16<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 1.3166\n",
      "Accuracy: 0.6448\n",
      "Confusion Matrix:\n",
      "[[350   1   2  19]\n",
      " [ 70   0   1   2]\n",
      " [ 30   0   0   1]\n",
      " [107   0   0  73]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training Loss: 1.1774\n",
      "Accuracy: 0.6784\n",
      "Confusion Matrix:\n",
      "[[353   0   4  15]\n",
      " [ 69   0   2   2]\n",
      " [ 24   0   1   6]\n",
      " [ 87   0   2  91]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training Loss: 1.0534\n",
      "Accuracy: 0.6982\n",
      "Confusion Matrix:\n",
      "[[361   0   0  11]\n",
      " [ 72   0   1   0]\n",
      " [ 29   0   1   1]\n",
      " [ 83   1   0  96]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:13<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training Loss: 0.9437\n",
      "Accuracy: 0.6997\n",
      "Confusion Matrix:\n",
      "[[363   0   0   9]\n",
      " [ 72   0   1   0]\n",
      " [ 28   0   2   1]\n",
      " [ 86   0   0  94]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training Loss: 0.8627\n",
      "Accuracy: 0.7088\n",
      "Confusion Matrix:\n",
      "[[358   0   0  14]\n",
      " [ 70   0   0   3]\n",
      " [ 24   0   2   5]\n",
      " [ 73   0   2 105]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training Loss: 0.8027\n",
      "Accuracy: 0.7256\n",
      "Confusion Matrix:\n",
      "[[362   0   0  10]\n",
      " [ 64   7   1   1]\n",
      " [ 30   0   0   1]\n",
      " [ 68   3   2 107]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training Loss: 0.7480\n",
      "Accuracy: 0.7241\n",
      "Confusion Matrix:\n",
      "[[354   8   0  10]\n",
      " [ 60  11   0   2]\n",
      " [ 27   2   1   1]\n",
      " [ 62   7   2 109]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training Loss: 0.7007\n",
      "Accuracy: 0.7409\n",
      "Confusion Matrix:\n",
      "[[356   6   0  10]\n",
      " [ 55  16   0   2]\n",
      " [ 26   2   2   1]\n",
      " [ 59   7   2 112]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training Loss: 0.6580\n",
      "Accuracy: 0.7348\n",
      "Confusion Matrix:\n",
      "[[355   8   0   9]\n",
      " [ 58  13   0   2]\n",
      " [ 27   2   1   1]\n",
      " [ 57   8   2 113]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training Loss: 0.6129\n",
      "Accuracy: 0.7591\n",
      "Confusion Matrix:\n",
      "[[354   6   0  12]\n",
      " [ 47  22   1   3]\n",
      " [ 24   3   2   2]\n",
      " [ 50   8   2 120]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training Loss: 0.5817\n",
      "Accuracy: 0.7576\n",
      "Confusion Matrix:\n",
      "[[347   9   0  16]\n",
      " [ 43  26   1   3]\n",
      " [ 22   5   1   3]\n",
      " [ 46   8   3 123]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:15<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training Loss: 0.5443\n",
      "Accuracy: 0.7591\n",
      "Confusion Matrix:\n",
      "[[359   6   0   7]\n",
      " [ 51  18   2   2]\n",
      " [ 23   4   2   2]\n",
      " [ 52   7   2 119]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:15<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training Loss: 0.4996\n",
      "Accuracy: 0.7591\n",
      "Confusion Matrix:\n",
      "[[360   5   0   7]\n",
      " [ 52  17   2   2]\n",
      " [ 25   2   1   3]\n",
      " [ 51   5   4 120]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training Loss: 0.4693\n",
      "Accuracy: 0.7729\n",
      "Confusion Matrix:\n",
      "[[358   4   0  10]\n",
      " [ 46  23   2   2]\n",
      " [ 19   6   3   3]\n",
      " [ 46   7   4 123]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training Loss: 0.4342\n",
      "Accuracy: 0.7713\n",
      "Confusion Matrix:\n",
      "[[359   7   0   6]\n",
      " [ 47  21   2   3]\n",
      " [ 19   5   4   3]\n",
      " [ 49   7   2 122]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training Loss: 0.4089\n",
      "Accuracy: 0.7759\n",
      "Confusion Matrix:\n",
      "[[348  10   0  14]\n",
      " [ 40  28   2   3]\n",
      " [ 19   6   2   4]\n",
      " [ 39   8   2 131]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:15<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training Loss: 0.3857\n",
      "Accuracy: 0.7835\n",
      "Confusion Matrix:\n",
      "[[356   7   0   9]\n",
      " [ 41  26   3   3]\n",
      " [ 17   6   5   3]\n",
      " [ 42   7   4 127]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:15<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training Loss: 0.3484\n",
      "Accuracy: 0.7866\n",
      "Confusion Matrix:\n",
      "[[347   9   0  16]\n",
      " [ 36  28   5   4]\n",
      " [ 15   5   6   5]\n",
      " [ 36   6   3 135]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:14<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training Loss: 0.3292\n",
      "Accuracy: 0.7835\n",
      "Confusion Matrix:\n",
      "[[354  11   0   7]\n",
      " [ 38  27   4   4]\n",
      " [ 18   6   4   3]\n",
      " [ 39   9   3 129]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:15<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training Loss: 0.3114\n",
      "Accuracy: 0.7912\n",
      "Confusion Matrix:\n",
      "[[350  10   1  11]\n",
      " [ 35  29   5   4]\n",
      " [ 14   5   7   5]\n",
      " [ 37   7   3 133]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "save_path = '.\\model.pth'\n",
    "best_accuracy = 79\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 移动数据到 CUDA\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = lossfunc(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Training Loss: {train_loss:.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  \n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            _, real_labels = torch.max(labels, 1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(real_labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] - Saved Best Model (Best Accuracy: {best_accuracy:.4f})')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total accuracy is: 0.7911585365853658\n",
      "\n",
      "The accuracy of mono type is: 0.7708333333333334\n",
      "\n",
      "The accuracy of poly type is: 0.8028846153846154\n",
      "\n",
      "The total confusion matrix is:\n",
      " [[350  10   1  11]\n",
      " [ 35  29   5   4]\n",
      " [ 14   5   7   5]\n",
      " [ 37   7   3 133]]\n",
      "\n",
      "The confusion matrix of mono type is:\n",
      " [[127   1   1   5]\n",
      " [ 17   9   2   1]\n",
      " [  5   2   3   2]\n",
      " [ 15   3   1  46]]\n",
      "\n",
      "The confusion matrix of poly type is:\n",
      " [[223   9   0   6]\n",
      " [ 18  20   3   3]\n",
      " [  9   3   4   3]\n",
      " [ 22   4   2  87]]\n",
      "\n",
      "The total F1 Score is: 0.607687427092892\n",
      "\n",
      "The F1 Score of mono type is: 0.5875846549402974\n",
      "\n",
      "The F1 Score of poly type is: 0.6183270504463467\n"
     ]
    }
   ],
   "source": [
    "# 创建两个子集，分别存储mono和poly的标签和预测\n",
    "mono_subset_labels = [label for label, img_type in zip(all_labels, types) if img_type == \"mono\"]\n",
    "mono_subset_predictions = [pred for pred, img_type in zip(all_predictions, types) if img_type == \"mono\"]\n",
    "poly_subset_labels = [label for label, img_type in zip(all_labels, types) if img_type == \"poly\"]\n",
    "poly_subset_predictions = [pred for pred, img_type in zip(all_predictions, types) if img_type == \"poly\"]\n",
    "\n",
    "# 计算不同类型的准确率和混淆矩阵\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "accuracy_mono = accuracy_score(mono_subset_labels, mono_subset_predictions)\n",
    "accuracy_poly = accuracy_score(poly_subset_labels, poly_subset_predictions)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "conf_matrix_mono = confusion_matrix(mono_subset_labels, mono_subset_predictions)\n",
    "conf_matrix_poly = confusion_matrix(poly_subset_labels, poly_subset_predictions)\n",
    "f1_total = f1_score(all_labels, all_predictions, average='macro')\n",
    "f1_mono = f1_score(mono_subset_labels, mono_subset_predictions, average='macro')\n",
    "f1_poly = f1_score(poly_subset_labels, poly_subset_predictions, average='macro')\n",
    "\n",
    "print('The total accuracy is:', accuracy)\n",
    "print()\n",
    "print('The accuracy of mono type is:', accuracy_mono)\n",
    "print()\n",
    "print('The accuracy of poly type is:', accuracy_poly)\n",
    "print()\n",
    "print('The total confusion matrix is:\\n', conf_matrix)\n",
    "print()\n",
    "print('The confusion matrix of mono type is:\\n', conf_matrix_mono)\n",
    "print()\n",
    "print('The confusion matrix of poly type is:\\n', conf_matrix_poly)\n",
    "print()\n",
    "print('The total F1 Score is:', f1_total)\n",
    "print()\n",
    "print('The F1 Score of mono type is:', f1_mono)\n",
    "print()\n",
    "print('The F1 Score of poly type is:', f1_poly)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
