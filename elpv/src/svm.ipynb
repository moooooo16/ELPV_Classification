{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd elpv/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/drive/MyDrive/project/elpv')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifier import *\n",
    "from utils.dataloader import *\n",
    "from utils.features import *\n",
    "from utils.helper import *\n",
    "from utils.processing import *\n",
    "from functools import partial\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moooooo16/Documents/Computer Vision/Project/elpv\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 66\n",
    "ROOT_PATH, DATA_PATH, OUT_PATH = get_paths()\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7] [588 117  56 313 920 178  50 402]\n",
      "2624 2624 2624 2624\n"
     ]
    }
   ],
   "source": [
    "img_path, prob, types, labels= load_data(DATA_PATH)\n",
    "print(len(img_path), \n",
    "      len(prob),\n",
    "      len(types),\n",
    "      len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class mapping \n",
    "\n",
    "|Label | prob | type|\n",
    "|------|------|-----|\n",
    "|0     | 0.0  | mono|\n",
    "|1     | 0.333| mono|\n",
    "|2     | 0.667| mono|\n",
    "|3     | 1.0  | mono|\n",
    "|4     | 0.0  | poly|\n",
    "|5     | 0.333| poly|\n",
    "|6     | 0.667| poly|\n",
    "|7     | 1.0  | poly|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1968, 300, 300) (1968,)\n",
      "(656, 300, 300) (656,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_builder = FeatureExtraction(DATA_PATH, img_path, labels)\n",
    "X_train, X_test, y_train, y_test = feature_builder.split_data(train_test_split, randome_state=RANDOM_STATE, stratify=True, split_ratio=0.25)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting images: 100%|██████████| 1968/1968 [00:00<00:00, 111742.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting done, added image: 900\n",
      "(2868, 300, 300) (2868,)\n",
      "(656, 300, 300) (656,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_aug, y_train_aug = feature_builder.augmentation(X_train, y_train, augment_funcs=[flip_x, flip_y, rotate])\n",
    "print(X_train_aug.shape, y_train_aug.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing images:   0%|          | 0/2868 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After clach_img: range 11 to 236\n",
      "After guassian_blur: range 24 to 221\n",
      "After lap_feature: range 0 to 10\n",
      "After min_max_normalize: range 0 to 255\n",
      "After morpo_opening: range 0 to 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing images: 100%|██████████| 2868/2868 [00:01<00:00, 1817.56it/s]\n",
      "Pre-processing images: 100%|██████████| 656/656 [00:00<00:00, 1820.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2868, 300, 300) (2868,)\n",
      "(656, 300, 300) (656,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clach_params = partial(clach_img, clipLimit=2.0, tileGridSize=(8, 8))\n",
    "gaussian_params = partial(guassian_blur, kernel_size=(0,0), sigmaX=3)\n",
    "lap_params = partial(lap_feature, dst = -1, ksize = 5)\n",
    "kernel = np.ones((2,2), np.uint8)\n",
    "morpo_opening_params = partial(morpo_opening, kernel=kernel, iterations=1)\n",
    "\n",
    "\n",
    "X_train_prepro = feature_builder.preprocess(X_train_aug, preprocess_funcs=[clach_params, gaussian_params, lap_params,min_max_normalize, morpo_opening_params])\n",
    "X_test_prepro = feature_builder.preprocess(X_test, preprocess_funcs=[clach_params, gaussian_params, lap_params,min_max_normalize, morpo_opening_params])\n",
    "print(X_train_prepro.shape, y_train_aug.shape)\n",
    "print(X_test_prepro.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating descriptors: 100%|██████████| 2868/2868 [00:24<00:00, 118.71it/s]\n",
      "Calculating descriptors: 100%|██████████| 656/656 [00:05<00:00, 125.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "train_kps, train_sift_descriptors, train_empty_desc = feature_builder.get_sift_descriptor(X_train_prepro)\n",
    "test_kps, test_sift_descriptors, test_empty_desc = feature_builder.get_sift_descriptor(X_test_prepro)\n",
    "\n",
    "if len(train_empty_desc) > 0:\n",
    "    print(\"Empty descriptor: \", train_empty_desc)\n",
    "if len(test_empty_desc) > 0:\n",
    "    print(\"Empty descriptor: \", test_empty_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating kmeans for k = 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building histogram for k = 80: 100%|██████████| 2868/2868 [00:04<00:00, 645.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating kmeans for k = 160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.11/site-packages/numpy/core/multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 346\u001b[0m \u001b[39m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[39m.\u001b[39mwhere)\n\u001b[1;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwhere\u001b[39m(condition, x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    348\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.11/site-packages/numpy/core/multiarray.py\", line 346, in where\n",
      "    @array_function_from_c_func_and_dispatcher(_multiarray_umath.where)\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "ks = [80, 160, 240, 320, 400]\n",
    "\n",
    "kmeans, train_sift_hists = feature_builder.build_sift_cluster(KMeans, train_sift_descriptors, ks, state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "test_sift_hists = {}\n",
    "for k in ks:\n",
    "    test_sift_hist = feature_builder.get_hist(test_sift_descriptors, kmeans[k], k)\n",
    "    test_sift_hists[k] = test_sift_hist\n",
    "\n",
    "print(train_sift_hists[ks[0]].shape)\n",
    "print(test_sift_hists[ks[0]].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hog_des = feature_builder.get_hog_features(X_train_prepro, orient=9, \n",
    "                                                 pix_per_cell=25, cell_per_block=2, \n",
    "                                                 vis=False, block_norm='L2-Hys')\n",
    "\n",
    "test_hog_des = feature_builder.get_hog_features(X_test_prepro, orient=9,\n",
    "                                                pix_per_cell=25, cell_per_block=2,\n",
    "                                                vis=False, block_norm='L2-Hys')\n",
    "\n",
    "print(train_hog_des.shape)\n",
    "print(test_hog_des.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUT_PATH, 'sift_X.pkl'), 'wb') as f:\n",
    "    pickle.dump((train_sift_hists, test_sift_hists), f)\n",
    "    \n",
    "with open(os.path.join(OUT_PATH, 'y.pkl'), 'wb') as f:\n",
    "    pickle.dump((y_train_aug, y_test), f)\n",
    "    \n",
    "with open(os.path.join(OUT_PATH, 'hog_X.pkl'), 'wb') as f:\n",
    "    pickle.dump((train_hog_des, test_hog_des), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "param = {\n",
    "    'kernel': ['rbf'],\n",
    "    'C' : [0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "\n",
    "clf_collections = {}\n",
    "predictions_collections = {}\n",
    "distance_collections = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = my_logger(os.path.join(OUT_PATH, 'logs/sift_svm.log'), 'sift_svm')\n",
    "logger.warning(f'k,i,j,best_score,,best_clf,mean_train_score,mean_test_score')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k in ks:\n",
    "    print(\"K: \", k)\n",
    "    total_classes = np.unique(y_train)\n",
    "    train_X = StandardScaler.fit_transform(train_sift_hists[k])\n",
    "    test_X = StandardScaler.fit_transform(test_sift_hists[k])\n",
    "    \n",
    "    \n",
    "    best_clfs = one_vs_other_up_sampling(X_train = train_X, y_train=y_train_aug, \n",
    "                                         total_classes= total_classes, clf = SVC, param = param, \n",
    "                                         k = k, logger = logger, \n",
    "                                         knn= KNeighborsClassifier, n_neighbors=5, verbose=3)\n",
    "\n",
    "    distance , pred = distance_vote(test_X, best_clfs)\n",
    "    acc, conf_mat = get_report(y_test, pred)\n",
    "    print()\n",
    "    print(f'Overall Test Accuracy: {acc:.2f}')\n",
    "    print(f'Confusion Matrix: \\n{conf_mat}')\n",
    "    print('-'*100)\n",
    "    \n",
    "    clf_collections[k] = best_clfs\n",
    "    predictions_collections[k] = pred\n",
    "    distance_collections[k] = distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = my_logger(os.path.join(OUT_PATH, 'logs/hog_svm.log'), 'hogsvm')\n",
    "logger.warning(f'i,j,best_score,,best_clf,mean_train_score,mean_test_score')\n",
    "\n",
    "\n",
    "train_X = StandardScaler.fit_transform(train_hog_des)\n",
    "test_X = StandardScaler.fit_transform(test_hog_des)\n",
    "\n",
    "\n",
    "total_classes = np.unique(y_train)\n",
    "k = None\n",
    "\n",
    "\n",
    "clfs = one_vs_other_up_sampling(total_classes, train_X, y_train_aug,\n",
    "                            clf = SVC,\n",
    "                            param=param, k=k, logger=logger,\n",
    "                            knn = KNeighborsClassifier, n_neighbors=5)\n",
    "\n",
    "\n",
    "\n",
    "distance , pred = distance_vote(test_X, clfs, None)\n",
    "\n",
    "acc, conf_mat = get_report(y_test, pred)\n",
    "print()\n",
    "print(f'Overall Test Accuracy: {acc:.2f}')\n",
    "print(f'Confusion Matrix: \\n{conf_mat}')\n",
    "print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
